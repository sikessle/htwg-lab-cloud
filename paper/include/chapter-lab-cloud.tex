\chapter{HTWG Lab Cloud}

Nachfolgend werden die Ziele, Anwendungsfälle und die eigentliche Implementierung des Projekts erläutert.
Der Quellcode ist öffentlich auf GitHub \cite{git-source} verfügbar. Weitere Entwicklungen finden stets in diesem Repository statt. Etwaige Pull-Request werden gerne angenommen.

Nachfolgend werden relevante Teile des Projektes dokumentiert und erklärt. Dabei liegt der Fokus auf einem Verständnis der grundlegenden Funktionsweise und Konzepte. Weitere detaillierte Instruktionen, die für Entwickler relevant sind, finden sich in den Verzeichnissen des Projekts in Form von \code{README.md} Dateien. Des Weiteren ist der Quelltext umfangreich mit Kommentaren versehen.

\section{Zielsetzung}
Erfahrungsgemäß kennt jeder Student das folgende Problem. Eine Vorlesung wird
besucht, in welcher neue Technologien kennengelernt werden. Folglich werden neue Anwendungen
eingesetzt, die zunächst installiert werden müssen. 
Findet die Vorlesung in einem Laborraum statt, 
so ist die notwendige Software meist schon installiert.
Dabei sind spezielle Anwendungen häufig nicht in allen Laboren vorzufinden. Folglich müsste
eine Vorlesung in einem entsprechenden Raum stattfinden.
Das Installieren der Software ist zudem ein umständlicher Prozess. Ein Dozent muss hierfür
das Rechenzentrum beauftragen, welches dann die notwendige Software installiert.
Folglich wäre es viel einfacher wenn ein Professor die für seine Vorlesung erforderliche Software
eigens zusammenstellen könnte.
Zudem wäre es für einen Studenten wünschenswert, den Zugriff auf die Software einer Vorlesung,
von jedem beliebigen Laborrechner zu erhalten.
Die HTWG Lab Cloud will die so eben beschriebenen Probleme lösen und einen Cloud Dienst zur
Verfügung stellen. Dieser Dienst soll das einfache Erstellen von virtuellen Maschinen für
Vorlesungen zur Verfügung stellen. Die somit erstellten virtuellen Maschinen sollen dann für
einen Studenten über eine entsprechende Remote Verbindung zugreifbar sein. Der Cloud Dienst soll
sich zudem in die vorhandenen Systeme integrieren. Damit ist gemeint, dass vorhandene
Dienste genutzt werden sollen wie beispielsweise Authentifizierungsmechanismen der Hochschule
oder auch die Verwaltung von Vorlesungen durch Moodle.

Die Ziele der HTWG Lab Cloud sind wie folgt definiert:
\begin{itemize}
\item Bereitstellen einer Konfiguration von virtuellen Maschinen für Professoren.
\item Starten von virtuellen Maschinen für die Teilnehmer einer Vorlesung.
\item Ein vom Laborraum unabhängiger Zugriff auf virtuelle Maschinen durch Studenten.
\item Die Nutzung der vorhandenen Authentifizierungsmethoden der Hochschule.
\item Integration von Moodle, um vorhandene Informationen zu nutzen.
\end{itemize}

\section{Workflow und Organisation}
Die nachfolgende Abbildung \autoref{overview} soll einen Überblick über 
die vorhandenen Komponenten der HTWG Lab Cloud geben.
Ein Professor kann über einen
Webbrowser auf das Dashboard der Cloud zugreifen. Der Zugriff auf das Dashboard wird dabei
über den LDAP Server der HTWG Konstanz geregelt. Nach erfolgreicher Authentifizierung 
zeigt das Dashboard die Vorlesungen eines Professors an. Die hierfür notwendigen Informationen
werden vom Moodle Server der Fachhochschule bezogen. 
Es besteht nun die Möglichkeit, virtuelle Maschinen für eine Vorlesung zu starten.
Dabei stehen dem Professor eine Reihe von Basis-Images bereit, welche er beliebig erweitern kann.
Nach erfolgreichem Start der Instanzen werden die
Studenten, welche in der Vorlesung angemeldet sind, über den HTWG Mail Server mit den
Zugriffsdaten ihrer virtuellen Maschine informiert. Der Zugriff auf die Maschine erfolgt dann mit
einem Browser über VNC. Zur Authentifizierung eines Nutzers auf einer virtuellen Maschine wird
dann wieder der LDAP Server der HTWG verwendet.
Weitere Informationen über die Authentifizierung über LDAP sind in den nachfolgenden
Kapiteln \ref{ldap_auth1} und \ref{ldap_auth2} zu finden.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{img/overview.pdf}
\caption{Überblick HTWG Lab Cloud}
\label{overview}
\FloatBarrier
\end{figure}

Nachdem wir einen Überblick des Systems betrachtet haben, sollen nun 
die internen Abläufe der HTWG Lab Cloud erläutert werden. Die folgenden
Vorgänge sind völlig transparent für einen Professor und Studenten und dienen lediglich zum
Verständnis der technischen Umsetzung.
Dabei soll die Kommunikation mit den notwendigen Openstack Komponenten verdeutlicht werden.
Also beispielsweise welche Komponenten für das Starten von virtuellen Maschinen Instanzen 
für eine Vorlesung notwendig sind.
In den nachfolgenden Grafiken wird zur Übersicht auf die Darstellung von
Authentifizierungsschritten verzichtet, wenngleich diese in der Implementierung vorzufinden sind.

Die in den folgenden Unterkapiteln angegebenen Arbeitsschritte wurden vollständig in Horizon
integriert, in Form einer Dashboard Erweiterung.
Zur Kommunikation mit den einzelnen Komponenten, wie beispielsweise Nova oder auch Cinder, wurden
die jeweiligen Python APIs verwendet.






\newpage
\subsection{Anzeigen von Vorlesungen}
Betrachten wir das Anzeigen der Vorlesungen eines Professors im Horizon Dashboard.
Hierfür wählt ein Professor zunächst die Übersichtssichtseite auf dem Dashboard aus.
Das Verwalten von virtuellen Maschinen geschieht in Openstack auf Basis eines
Tenants. Der Begriff Tenant ist dabei äquivalent zu einem Projekt. 
Auf ein Tenant können Openstack Nutzer Zugriff erhalten, um virtuelle Maschinen starten
zu können. Dabei können Tenants von einem Administrator entsprechend konfiguriert werden, damit
beispielsweise nur eine festgelegte Anzahl von Ressourcen verwendet werden kann.

Um nun virtuelle Maschinen verwalten zu können, muss also ein Tenant existieren.
Damit die HTWG Lab Cloud in die vorhandenen Systeme integriert wird, werden zunächst
alle Vorlesungen des angemeldeten Professors vom HTWG Moodle Server geladen.
Daraufhin wird für jede Vorlesung ein gleichnamiger Tenant erstellt. Zuletzt wird der Tenant
mit Zugriffsrechten entsprechend konfiguriert. Die beschriebenen Schritte sind unter der folgenden
Abbildung \ref{createTenants} dargestellt.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{img/createTenants.pdf}
\caption{Vorlesungen in der Cloud}
\label{createTenants}
\FloatBarrier
\end{figure}


\subsection{Starten von Instanzen}
Beim Starten der Instanzen einer Vorlesung 
(siehe Abbildung \ref{startInstances}) setzten wir voraus das für
die jeweiligen Vorlesungen bereits ein Tenant existiert. Wählen wir nun eine Vorlesung aus
(siehe hierzu auch Kapitel \ref{horizon}), so wird zuerst eine Teilnehmerliste vom HTWG Moodle Server
angefragt. Dabei enthält die Teilnehmerliste die Namen und Email Adressen der Studenten.
Nun wird für jeden Teilnehmer eine Instanz gestartet. Die Instanz wird dabei für das jeweilige
Tenant gestartet und folgt einem vorgegebenen Namensschema. Das Schema ist eine Kombination
aus der Email Adresse des Teilnehmers und dem Namen des Tenants. Durch die eindeutige Namensvergabe
wird sichergestellt das nicht mehrere Instanzen für einen Studenten in einem Tenant gestartet
werden können.
Die Instanz wird dabei über Nova gestartet. Nach dem Starten der Instanz wird noch ein Blockgerät
über Cinder angelegt, welches daraufhin mit der jeweiligen Instanz verbunden wird. Dadurch können
Studenten ihre Daten ablegen.
Zuletzt wird dem Studenten eine Email mittels dem HTWG SMTP Server übermittelt, welche einen
Link für den Zugriff auf seine virtuelle Maschine über VNC enthält.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{img/startInstances.pdf}
\caption{Instanzen für eine Vorlesung starten}
\label{startInstances}
\FloatBarrier
\end{figure}

\subsection{Stoppen von Instanzen}
Das Stoppen der virtuellen Maschinen, welches unter Abbildung \ref{stopInstances} dargestellt ist,
soll hiermit auch erwähnt werden.
Beim Stoppen wird nicht zuerst eine Teilnehmerliste vom HTWG Moodle Server angefragt, wie
wir es noch beim Starten der Instanzen gesehen haben. Stattdessen wird über Nova eine Anfrage
gestellt, um alle Instanzen des jeweiligen Tenants zu erhalten. Die so erhaltenen Instanzen werden
daraufhin gelöscht. Dasselbe wird mit den Blockgeräten gemacht, welche beim Starten einer
Instanz erstellt wurden. Für die Blockgeräte Verwaltung ist Cinder zuständig.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{img/stopInstances.pdf}
\caption{Instanzen einer Vorlesung stoppen}
\label{stopInstances}
\FloatBarrier
\end{figure}

\subsection{Aktualisieren von Instanzen}
Treten einem Moodle Kurs Studenten bei, nachdem die virtuellen Maschinen für die
Teilnehmer des Kurses bereits gestartet wurden, so besitzen die neu hinzugekommenen Studenten noch
keine Instanz. Aus diesem Grund kann für einen Kurs die Aktualisieren-Funktion aufgerufen werden,
welche für jeden neu hinzugekommen Studenten eine Instanz startet. 
Dabei ist der Arbeitsablauf analog zum Starten von Instanzen wie unter Abbildung 
\ref{startInstances} bereits beschrieben.

\section{Dashboard}

Das unter dem Codenamen Horizon entwickelte webbasierte Dashboard vereint alle OpenStack-Komponenten, die zur Erstellung und Verwaltung einer Cloud-Umgebung erforderlich sind. Gegenüber der Kommandozeilen-Variante bietet Horizon eine komfortable Verwaltung der Cloud-Ressourcen und unterstützt die Durchführung der wichtigsten administrativen Verwaltungsaufgaben. Dazu zählen unter anderem:
\begin{itemize}
\item Integration von Images
\item Management der Instanzen
\item Verwaltung der Volumes
\item Einrichtung von Security Rules
\item Logs und Reports
\item VNC
\item etc.
\end{itemize}
Das Dashboard wurde in der Programmiersprache Python implementiert und nutzt neben dem Django-Framework für Webapplikationen auch Apache 2.0 als Webserver. Aufgrund des modularen Aufbaus von OpenStack kann Horizon individuell angepasst und erweitert werden. Die zugehörigen Schritte zur Anpassung des Frontends und Umsetzung einer funktionellen Erweiterung sind nachfolgend beschrieben.

\subsection{Anpassung von Horizon}

Zur Vereinheitlichung des Designs wurde das Dashboard an das HTWG-Design angepasst. Somit fügt sich die HTWG Lab Cloud nahtlos in das offizielle HTWG Design ein.

\begin{figure}[H]
\centering
\includegraphics[scale=0.38]{img/dashboard-login.png}
\caption{HTWG Lab Cloud Dashboard Login}
\label{dashboard-login}
\end{figure}

Der Anmeldebildschirm zeigt das HTWG Lab Cloud Logo (vgl. \autoref{dashboard-login}).
Die Blautöne entsprechen denen der HTWG.
Ebenso wurden die anderen Ansichten im Dashboard alle an das HTWG Design angepasst (vgl. \autoref{dashboard-overview}).

\begin{figure}[H]
\centering
\includegraphics[scale=0.43]{img/dashboard-overview.png}
\caption{Übersichtsseite HTWG Lab Cloud Dashboard}
\label{dashboard-overview}
\end{figure}

Realisiert wurde die Anpassung mittels SCSS\footnote{Sass-CSS, eine erweiterte CSS Sprache.} und dem Theme-Mechanismus von Horizon.
Dabei müssen lediglich eine Datei und der \code{lab-cloud} Theme-Ordner in das Horizon Verzeichnis kopiert werden.
Ein anschließendes \code{python manage.py collectstatic --noinput --clear && python manage.py compress --force && sudo service apache2 restart} im Horizon Ordner kompiliert die Themes und startet den Webserver neu.

\subsection{Erweiterung von Horizon}\label{horizon}
Zur vereinfachten Erstellung der virtuellen Maschinen für entsprechende Vorlesungen wurde Horizon um zusätzliche Funktionalität erweitert. Dazu zählt sowohl das Anzeigen aller Moodle-Kurse, in denen der Professor angemeldet ist, als auch die webbasierte Visualisierung zum Starten und Stoppen der entsprechenden Instanzen. Bevor auf die Herangehensweise zur Implementierung der Funktionalität eingangen wird, sollen zunächst die einzelnen Komponenten des Dashboards, die für das weitere Verständnis grundlegend sind, kurz erläutert werden. 

Als Dashboard wird sowohl das gesamte Webinterface von Horizon als auch die Top-Level-Komponente bezeichnet, die PanelGroups und Panels beinhaltet. Sogenannte PanelGroups dienen wiederum zur Gruppierung einzelner Panels. Während es sich bei Dashboards und PanelGroups ausschließlich um Navigationselemente handelt, bildet ein Panel die Grundlage einer Seite. Mit Hilfe von TabGroups und Tabs kann der Inhalt eines Panels kategorisch unterteilt werden. In Horizon wird ein wesentlicher Teil der Daten durch DataTables übersichtlich in Tabellenform dargestellt. Diese werden dynamisch in der Weboberfläche aufgebaut und automatisch gerendert. Neben dem Filtern der Tabellendaten nach bestimmten Kriterien bietet die Komponente eine Vielzahl von Actions, wie beispielsweise Lösch- und Update-Mechanismen sowie Batch-Actions, die auf die Tabelle angewandt werden können. Einige dieser Actions ermöglichen das Ausführen von Dialogfeldern, sogenannten Workflows, die durch Benutzereingaben komplexe Prozesse abbilden. Zusätzlich zu den bereits beschriebenen Komponenten gibt es noch Templates, die für das Rendern der Seiten zuständig sind.

Da Horizon eine strikte Einhaltung der Namenskonventionen verfolgt, bestand der erste Schritt bei der Erstellung eines Dashboards sowie eines Panels in der Generierung einer Horizon-konformen Ordnerstruktur. Die Entwicklungsumgebung Devstack liefert standardmäßig ein Skript mit, das diese Problematik automatisch erledigt. Mit Hilfe der beiden Befehle \code{./run_tests.sh -m startdash prof --target openstack_dashboard/dashboards/prof} und \code{./run_tests.sh -m startpanel courses } \\ \code{--dashboard=openstack_dashboard.dashboards.prof  --target=openstack_dashboard/dashboards/prof/courses} erfolgte die Generierung der Ordnerstruktur für ein Dashboard \textit{Professor} und ein Panel \textit{Kurse}. Um das Dashboard anschließend anzuzeigen, musste es über die Datei \code{openstack_dashboard/enabled/_50_prof.py} anhand entsprechender Konfiguationsparameter aktiviert und Horizon zugänglich gemacht werden.

Für das Anzeigen der Moodle-Kurse, in denen ein Professor angemeldet ist, wurde eine DataTable erstellt, die sämtliche Informationen der zugehörigen Vorlesungen übersichtlich darstellt. Auf dieser Übersichtsseite wird dem Professor anhand unterschiedlicher Aktionen die Möglichkeit geboten, Instanzen zu starten und zu stoppen, Vorlesungen zu filtern sowie Kurse zu aktualisieren. Bedingt dadurch, dass sich entsprechende Kursteilnehmer erst im Nachhinein in dem entsprechenden Moodle-Kurs angemeldet haben und dementsprechend keiner Instanz zugewiesen sind, dient letztere Aktion dazu, die fehlenden Instanzen nachträglich zu starten. Abbildung \ref{dashboard-courses} zeigt die Übersichtsseite der aus Moodle aufgelisteten Kurse.

\begin{figure}[H]
\centering
\includegraphics[scale=0.18]{img/dashboard-courses.png}
\caption{Übersichtsseite Kurse}
\label{dashboard-courses}
\end{figure}

Das Starten der Instanzen wurde mittels eines Workflows realisiert, das unterschiedliche Auswahlmöglichkeiten für den Professor beinhaltet (Abbildung \ref{dashboard-start-instances}). Neben der Verfügbarkeitszone und der Variante ist vorallem die Boot-Quelle essentiell, um die Instanzen entweder von einem Abbild oder einem Snapshot zu starten. Nach dem Bestätigen des Starten-Buttons werden in Abhängigkeit der Anzahl der Kursteilnehmer die Instanzen gestartet und entsprechend zugewiesen.

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{img/dashboard-start-instances.png}
\caption{Workflow zum Starten der Instanzen}
\label{dashboard-start-instances}
\end{figure}

\section{OpenStack LDAP-Authentifizierung}\label{ldap_auth1}
Funktionalität wie beispielsweise Authentifizierung werden in OpenStack durch den Keystone Service bereitgestellt.
Keystone besteht intern aus mehreren internen Services. Bei diesen Services handelt es sich um den Identity Service, welcher für die Authentifizierung zuständig ist. Benutzer melden sich hierbei mit Name und Passwort an und erhalten nach erfolgreicher Authentifizierung ein Token, mit dem sie sich bei Aufrufen anderer Dienste authentifizieren können. Ebenso nutzen Services über eigene Service Benutzer den Identity Service, um aufrufende Benutzer zu authentifizieren. Token werden vom Token Service verwaltet und vergeben. Dieser Service ist ebenso für die Validierung von Token zuständig.

Des Weiteren verfügt Keystone über den Assignment Service. Dieser verwaltet Rollen und deren Mapping auf Benutzer, Projekte und Domänen. Projekte und Domänen werden wiederum durch den Resource Service verwaltet.

Darüber hinaus beinhaltet Keystone noch den Catalog Service, welcher ein Endpoint Register verwaltet, über das verwendete Endpoints gefunden und auf diese zugegriffen werden kann. Die Authentisierung wird wiederum durch den Policy Service verwaltet. Hierbei können Regeln entweder global, oder auf Ebene von einzelnen Services definiert werden.

Keystone bildet für diese internen Service eine Abstraktionsschicht. Diese bietet nach außen hin ein Interface, über das die Funktionen aufgerufen werden können und leitet die eingehenden Aufrufe zu den eigentlichen Services weiter. Diese Services können wiederum ein oder mehrere Backends besitzen, die die eigentliche Ausführung tätigen. Initial handelt es sich hierbei um SQL Datenbanken, in denen die entsprechenden Daten gespeichert werden.

Da der Kreis der Benutzer der HTWG Lab Cloud auf die Professoren der HTWG Konstanz beschränkt sein soll und die Benutzung des Systems mit möglichst wenig Aufwand verbunden sein soll, wird der an der Hochschule bereits vorhandene LDAP Server zum Zwecke der Authentifizierung als Backend für den Keystone Identity Service verwendet.
Es besteht auch die Möglichkeit, einen LDAP Server als Backend sowohl für den Identity, als auch für den Assignment Service zu verwenden. Allerdings besteht diese Möglichkeit an der HTWG nicht, da auf den Server nur lesend zugegriffen werden kann. Daher verbleibt dieser Service in seiner ursprünglichen Konfiguration mit einer SQL Datenbank als Backend.

Um einen LDAP Server als Backend des Identity Services zu verwenden, muss lediglich die Keystone Konfiguration (\emph{keystone.conf}) angepasst werden. Hierbei muss der SQL Treiber durch den LDAP Treiber ersetzt werden:

\code{[identity]}\\
\code{driver = keystone.identity.backends.sql.Identity}\\
\code{driver = keystone.identity.backends.ldap.Identity}

Zudem müssen die URL des HTWG LDAP Servers, die BaseDN und die Suchpfade für Benutzer und Gruppen angegeben werden:

\code{[ldap]}\\
\code{url = ldap://ldap.htwg-konstanz.de:389}\\
\code{suffix = dc=fh-konstanz,dc=de}\\
\code{user\_tree\_dn = ou=users,dc=fh-konstanz,dc=de}\\
\code{group\_tree\_dn = ou=groups,dc=fh-konstanz,dc=de}

Um Keystone eine korrekte Verarbeitung von Anfragen zu ermöglichen, müssen die Attribute des HTWG LDAP Servers auf die entsprechenden Attribute aus Keystone gemapped werden:

\code{user\_objectclass = FHKNPERSON}\\
\code{user\_id\_attribute = uidNumber}\\
\code{user\_name\_attribute = uid}\\
\code{user\_mail\_attribute = mail}\\
\code{group\_id\_attribute = gidNumber}

Bedingt durch den lediglich lesenden Zugriff auf den LDAP Server, müssen zudem die Optionen zum Erstellen, Aktualisieren und Löschen von Benutzern und Gruppen deaktiviert werden:

\code{user\_allow\_create = false}\\
\code{user\_allow\_update = false}\\
\code{user\_allow\_delete = false}

\code{group\_allow\_create = false}\\
\code{group\_allow\_update = false}\\
\code{group\_allow\_delete = false}

Teil der OpenStack seitigen Installation ist die Erstellungen von bestimmten Service Benutzern. Dies sind Benutzer, die von den Services wie beispielsweise Swift, Cinder, oder Neutron für Anfragen an andere Services genutzt werden. Da dies vor dem Wechsel des Identity Backends geschieht und auch keine Schreibrechte für den HTWG LDAP Server vorhanden sind, existieren diese Service Benutzer im späteren System nicht. Daher wurden diese Service Benutzer im LDAP der HTWG durch das Rechenzentrum angelegt. Diese Benutzer existieren dort vorerst bis zum 30.07.2016. Es handelt sich hierbei um folgende Benutzer:
\emph{glance, nova, cinder, neutron, swift, heat, heat-cfn, opnstadm}. (Hinweis des Rechenzentrums: Falls sich eine andere Möglichkeit der Verwaltung dieser Service Benutzer ergibt, sollten die Service Benutzer an das Rechenzentrum zu deren Verfügung zurückgegeben werden.)

\section{Anbindung an Moodle}
Bei der Entwicklung der HTWG Lab Cloud wurde großer Wert auf eine möglichst einfache Benutzung gelegt. Um dies zu gewährleisten, wurde das System so entworfen, dass die Professoren lediglich wenige Mausklicks benötigen um ihre Kurse im System zu erstellen.

An der HTWG Konstanz wird das E-Learning System Moodle zur Verwaltung und Durchführung von Veranstaltungen eingesetzt. Bei Moodle handelt es sich um ein Open-Source Projekt, welches durch die australischen Firma Moodle HQ verwaltet und geführt wird. Entwickelt wird dieses System von einer weltweiten Community.

Moodle bietet verschiedene Schnittstellen in Form von Web Services zum Zugriff und zur Bearbeitung von Daten an. Der Zugriff durch die HTWG Lab Cloud erfolgt über die REST Schnittstelle. Diese liefert die Ergebnisse der Anfrage in einem generischen XML Format zurück.
Um die bereits erwähnte möglichst einfache Benutzung umzusetzen, werden die Kurse des jeweils eingeloggten Professors, sowie deren Teilnehmer abgefragt. Somit können die entsprechende Projekte in OpenStack automatisch angelegt und die an den Kursen teilnehmenden Studierende benachrichtigt werden.

Anfragen über die REST Schnittstelle Moodles erfordern wiederum eine Authentifizierung des anfragenden Benutzers durch Moodle. Die einfachste Lösung wäre es, diese Anfrage im Namen des eingeloggten Professors durchzuführen, da dieser zudem über ausreichende Berechtigungen hierfür verfügt. Da durch Keystone lediglich der Benutzername und nicht das Passwort verfügbar sind, werden diese Anfragen stattdessen von einem speziellen Moodle Benutzer durchgeführt.

Moodle verwendet zwar, ebenso wie die HTWG Lab Cloud, den HTWG LDAP Server zur Authentifizierung der Benutzer, nimmt aber intern ein weiteres Mapping von Benutzernamen auf Moodle eigene IDs vor. Da die Anfragen über die REST Schnittstelle lediglich mit der Moodle ID des Professors und nicht mit dessen LDAP Benutzernamen möglich ist, muss diese Moodle ID in einem vorherigen Schritt per Suchanfrage über dessen E-Mail Adresse ermittelt werden. Daraufhin können dann die Kurse des eingeloggten Professors und deren jeweilige Teilnehmer ermittelt werden. Es werden hierbei lediglich die Kurse berücksichtigt, in denen der Professor tatsächlich der Leiter des Kurses ist. Dies ergibt folgenden Ablauf der Anfragen:

URL für den Login des Moodle Users:\\
\code{https://moodle.htwg-konstanz.de/moodle/login/token.php}

Parameter für den Login des speziellen Moodle Users (liefert Token für folgende Anfragen):\\
\code{username:ldap_userid}
\code{password:password}
\code{service:htwgapp}

Die Anfragen werden alle an folgende URL gesendet:\\
\code{https://moodle.htwg-konstanz.de/moodle/webservice/rest/server.php}

Parameter für die Suchanfrage zur Ermittlung der Moodle ID des eingeloggten Professors:\\
\code{stoken:token}\\
\code{wsfunction:core_user_get_users}\\
\code{criteria[0][key]:email}\\
\code{criteria[0][value]:ldap_user_email}

Parameter für die Abfrage der Kurse des eingeloggten Professors:\\
\code{wstoken:token}\\
\code{wsfunction:core_enrol_get_users_courses}\\
\code{userid:moodle_userid}

Parameter für die Abfrage der Teilnehmer eines Kurses:\\
\code{wstoken:token}\\
\code{wsfunction:core_enrol_get_enrolled_users}\\
\code{courseid:course_id}

Bedingt durch die Architektur von Horizon und Keystone muss einem Benutzer beim Login in Horizon mindestens ein Projekt zugeordnet sein. Das ist im Standard Anwendungsfall problemlos möglich, da die Benutzer in OpenStack erstellt werden und somit die Zuordnung eines Projekts erzwungen wird.

Kommt, wie im Falle der HTWG Lab Cloud, allerdings eine bereits vorhandene Menge von Benutzern zum Einsatz, so muss diese Zuordnung anderweitig vorgenommen werden. Dies kann nicht vom Benutzer selbst durchgeführt werden, da dieser sich, um ein Projekt zu erstellen und sich selbst zuzuordnen, einloggen müssten, was aber ohne zugeordnetes Projekt nicht möglich ist. Daher werden alle vorhandenen Professoren während der Installation der HTWG Lab Cloud über den LDAP Server ermittelt und einem Standard Projekt zugeordnet.

Um zu gewährleisten, dass auch während der Benutzung neu hinzugekommene Professoren berücksichtigt werden, wird dieser Schritt in regelmäßigen Zeitabständen für diese Untermenge wiederholt.

\section{Fernzugriff (VNC)}\label{vnc}

Meist wird in OpenStack Umgebungen lediglich von Instanzen ohne graphische Oberfläche ausgegangen, auf denen beliebige Serveranwendungen laufen. 
Der dadurch oft genannte SSH-Zugang ist dafür vollkommen ausreichend.
Die HTWG Lab Cloud stellt jedoch eher eine \emph{Virtual Desktop Infrastructure} bereit.
Benutzer, insbesondere von nicht-technischen Studiengängen erwarten die gewohnte graphische Desktopumgebung (z. B. Unity oder KDE) in dieser Cloud-Lösung.

Da VNC ein unsicheres Protokoll ist, sollte es stets durch einen gesicherten Tunnel hindurch verwendet werden.
Der OpenStack VNC Console Proxy bietet diese Funktionen an.
Um die URL für den VNC Server zu erhalten, erfolgt der Aufruf \code{nova get-vnc-console server_id novnc}.
Anschließend kann auf die Instanz über den Browser zugegriffen werden.
Dieser Browser-basierte VNC Zugang ist schneller als herkömmliche VNC Clients.

\section{Schattenkopie}

Das hier vorgestellte Basis-Image stellt lediglich die Grundfunktionalitäten bereit. 
Um maximalen Nutzen aus der HTWG Lab Cloud zu ziehen, ist es elementar das Image an die jeweilige Übung möglichst genau anzupassen.

Hierbei wurden die Boardmittel von OpenStack evaluiert.
Wie sich herausgestellt hat, bietet OpenStack mit der \emph{Schattenkopie} Funktionalität genau dieses Feature an.
Von einer erstellten Instanz kann jederzeit eine Schattenkopie angefertigt werden.
Diese entspricht exakt einem neuen herkömmlichen Image.

Also kann ein Professor das Basis-Image in einer Instanz starten, an die Übungsstunde anpassen und über \emph{Schattenkopie erstellen} ein neues Image erstellen.
Anschließend wird das Image beim Start der Instanzen ausgewählt.

\section{Netzwerk}

Die Netzwerkstruktur basiert auf der \emph{FlatDHCP} Struktur von nova-network. 
Dabei werden drei Netzwerkadapter mit folgender Konfiguration verwendet:

\begin{itemize}
\item eth0: NAT Interface, welches den Internetzugriff bereitstellt. Auf diesem Adapter läuft auch ein DHCP Server.
\item eth1: Interface ohne DHCP und Internetzugriff. Wird als Management-Netzwerk bzw. internes Netzwerk verwendet, über das die einzelnen Instanzen miteinander kommunizieren können. Muss sich im Promiscous Modus befinden, damit die Netzwerkpakete korrekt weitergereicht werden. Bei Installation in einer virtuellen Maschine ist dieser Adapter auf \emph{host-only} geschaltet.
\item eth2: Interface auf dem öffentliche IP-Adressen der Instanzen allokiert werden und das HTWG Lab Cloud Dashboard erreichbar ist. Bei Installation in einer virtuellen Maschine ist dieser Adapter mit dem Modus \emph{bridged}  mit einem lokalen, öffentlich zugänglichen Adapter verbunden.
\end{itemize}

Zusätzlich wird automatisch eine Bridge erstellt, welche die Instanzen (an den Adaptern vnetX) mit dem Adapter eth1 verbindet.
Bei Installation außerhalb einer virtuellen Maschine, kann der Adapter eth2 und eth0 zusammengelegt werden. 
Diese Trennung ist lediglich in einer virtuellen Maschine erforderlich, um von außerhalb auf die HTWG Lab Cloud bzw. die Instanzen zugreifen zu können.
Denn der Adapter eth2 stellt den Zugang von außerhalb über den bridged Modus zur Verfügung.
Des Weiteren muss auf dem Host noch mittels \code{iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE} Source-NAT aktiviert werden, sodass die Instanzen Internetzugriff haben.

\section{Compute Image}

Da die OpenStack Umgebung gewisse Anforderungen an das Instanz-Image stellt, wurde in diesem Projekt ein Basis-Image entwickelt. 
Dieses kann in der OpenStack Umgebung gestartet und vom Professor für die jeweilige Übung angepasst werden.
Grundsätzlich sind in OpenStack Windows XP, 7, 8 und Linux-basierte Betriebssysteme als Instanz konfigurierbar. 
Aus lizenzrechtlichen Gründen wurde die Entscheidung für das Linux-basierte Betriebssystem \emph{Ubuntu} getroffen. 
Prinzipiell lassen sich die hier durchgeführten Anpassungen auch auf Windows übertragen, wobei die konkrete Konfiguration abweicht.

Der Workflow, um ein Image anzupassen und nachfolgend Instanzen mit diesem den Studenten bereitzustellen, ergibt sich wie folgt:

\begin{enumerate}
\item Starten des Basis-Images in einer Instanz durch den Professor.
\item Installation von Programmen, Konfiguration der Instanz und andere Anpassungen.
\item Erstellen einer Schattenkopie, um die Änderungen festzuhalten. Dies erzeugt ein neues Image.
\item Starten der einzelnen Instanzen für die Studenten. Dabei wird die Instanz automatisch an den Student angepasst, sodass nur dieser Zugriff mittels seiner im LDAP hinterlegten Zugangsdaten hat.
\end{enumerate}

Im Folgenden wird auf die einzelnen Komponenten, die im Zuge dieser Projektarbeit erarbeitet wurden, eingegangen. 

\subsection{Betriebssystem Details}

Als Betriebssystem wurde Ubuntu 14.04 Desktop LTS ausgewählt. 
Dieses ist zwar nicht die neuste Ubuntu-Version, bietet jedoch aufgrund des \emph{Long Term Support} Updates und relevante Sicherheitsfixes für 5 Jahre. 
Somit ist gewährleistet, dass das Basis-Image bis 2019 mit relevanten Updates versorgt wird. 
Insbesondere unter Betrachtung des Support-Aufwands durch das HTWG-Rechenzentrums soll so eine lange Laufzeit gewährleistet werden.

Aufgrund der Dateigröße ($> 5$ GB) kann das Image nicht in Git eingecheckt werden und wird über Dropbox \cite{dropboxImage} bereitgestellt. Das Image kann auch direkt bei den Autoren der Projektarbeit angefordert werden.

Da das Basis-Image noch keine personenbezogenen Daten enthält, wird ein Standard-Account mit \code{sudo} Rechten bereitgestellt:

\begin{itemize}
\item \emph{Benutzer:} ubuntu (Vollständiger Name: HTWG Lab Cloud)
\item \emph{Passwort:} ubuntu
\end{itemize}

Das von uns bereitgestellte Image enthält bereits einige Standardprogramme:

\begin{itemize}
\item Unity als Desktop-Manager
\item Dropbox
\item git
\item openssh-server
\item vim
\item OpenOffice
\item Firefox
\item und weitere Office-Applikationen
\end{itemize}

Auf andere Cloudspeicher-Anbieter musste verzichtet werden, da Google Drive und Windows OneDrive Linux nicht nativ unterstützen.

\subsection{Cloud-Kompatibilität}

OpenStack stellt gewisse Anforderungen an die Gast-Betriebssysteme, die sich im OpenStack Handbuch \cite{osImageRequirement} nachschlagen lassen. 
Die initiale Konfiguration des Basis-Images wurde aus Performance-Gründen in VirtualBox durchgeführt.
Durch Installation einiger Pakete und Anpassung von Konfigurationsdateien, wurde das Ubuntu Image OpenStack kompatibel gemacht:

\begin{itemize}
\item Installation von \code{cloud-init}, \code{cloud-utils} und \code{cloud-initramfs-growroot} *
\item Anpassen der Kernel-Boot-Parameter in der Datei \code{/etc/default/grub}. Dort wurde der Parameter \code{console=tty0,ttyS0} ergänzt. Ein anschließendes \code{update-grub} aktualisert den Grub-Bootloader. Diese Änderung aktiviert die Ausgabe der Log-Meldungen im Horizon-Dashboard.
\item Anpassen von \code{cloud.cfg} des Pakets \code{cloud-init}. Dieses deaktiviert standardmäßig den Passwort-Login des Images. Da sich Nutzer jedoch per LDAP bzw. mit dem Standardbenutzername anmelden können sollen, muss dies aktiviert werden. *
\end{itemize}

Die mit * gekennzeichneten Punkte werden von dem entworfenen Skript \code{cloud-init-script.sh} automatisch ausgeführt. 
Anschließend löscht sich das Skript und fährt das Betriebssystem herunter.
Da nach Ausführung des \code{cloud-init-script.sh} Skripts das Image in VirtualBox nicht mehr ohne Weiteres gestartet werden kann, sollte das Skript am Ende der Konfiguration ausgeführt werden.

\subsection{Konvertierung von VirtualBox nach OpenStack}

Nachdem das Basis-Image konfiguriert und kompatibel zu OpenStack gemacht wurde, muss das Image noch in ein Format gewandelt werden, das OpenStack versteht.
Das entwickelte Skript \code{image-virtualbox-to-openstack.sh} führt diese Umwandlung aus. 
Die Benutzung erfolgt mit folgendem Aufruf: \code{./image-virtualbox-to-openstack.sh <Pfad zum gewünschten VirtualBox Image Snapshot>}.
Im Wesentlichen wird dabei mittels \code{VBoxManage} das Image in ein \code{raw} Format gewandelt und anschließend mit \code{qemu-img} nach \code{qcow2} überführt.

\subsection{LDAP-Authentifizierung}\label{ldap_auth2}

Studenten sollen sich mit ihren gewohnten HTWG Zugangsdaten bei den Instanzen anmelden können. 
Deshalb wird auf den LDAP-Server der HTWG zugegriffen, um den Benutzer beim Login in das Betriebssystem zu authentisieren. 
Da lediglich lesender Zugriff auf den LDAP-Server notwendig ist, sind keine speziellen Rechte erforderlich, um  mit dem LDAP-Server zu kommunizieren (insbesondere ist kein \emph{Bind DN} erforderlich).

Der HTWG LDAP-Server ist unter \url{ldap.htwg-konstanz.de} über die LDAP-Version 3 im Hochschulnetz erreichbar. 
Um auf das Wurzelverzeichnis zugreifen zu können, muss der \emph{Base DN} \code{dc=fh-konstanz,dc=de} gewählt werden. 
Einzelne Benutzer lassen sich mit dem DN \code{ou=users,dc=fh-konstanz,dc=de} auslesen, wobei die ungefilterte Anzeige auf 50 Benutzer begrenzt ist. 

Für das Projekt sind die Attribute

\begin{itemize}
\item uid: Benutzername
\item uidNumber: Benutzernummer
\item gidNumber: Gruppennummer
\item homeDirectory: Pfad zum home-Laufwerk
\item loginShell: Die für den Login genutzte Shell
\end{itemize}

relevant. Alle Attribute finden sich im HTWG LDAP-Server wieder und müssen somit nicht explizit gemappt werden. 
Eine Besonderheit besteht darin, dass die Attribute uidNumber, gidNumber, homeDirectory und loginShell bereits mit HTWG-spezifischen Werten belegt sind. 
So ist z. B. homeDirectory mit \enquote{/rzhome/username} vorbelegt.
Diese Werte lassen sich nicht für dieses Projekt verwenden.
Um trotzdem mit der HTWG interoperabel zu bleiben, werden diese Einträge beim Auslesen mittels \code{nss_override_attribute_value} überschrieben. 
Dabei werden jedoch im LDAP-Server \emph{keine} Einträge verändert, lediglich dem lokalen System erscheint der Eintrag als modifiziert.

Die Benutzer- und Gruppennummer wird auf fix auf 2000 gesetzt.
Damit ist genügend Raum für weitere lokale Benutzer, die eventuell von Programmen angelegt werden und automatisch bei 1001 starten.
Benutzer werden die Gruppen <username>, adm, cdrom, sudo, dip, plugdev, lpadmin, sambashare zugewiesen. 
Diese entsprechen den Ubuntu Standardgruppen eines neuen Benutzers.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.6]{img/lightdm.png}
\caption{Grafischer Login mit LightDM}
\label{lightdm}
\end{figure}

Betriebssystemseitig müssen die Pakete \code{ldap-auth-client} und \code{nscd} installiert werden. 
Die Authentifizierung wird mit dem Pluggable Authentication Modules (PAM) Framework realisiert. 
Mit dessen Hilfe lässt sich die Authentifizierung vom üblichen System abkoppeln und in ein LDAP-Modul auslagern.

Sicherheitsrelevant ist außerdem, dass eine einzelne Instanz nur von einem Benutzer erreichbar ist. 
Andernfalls besteht Gefahr, dass sich andere Nutzer auf der Instanz mit ihren Zugangsdaten anmelden und mittels \code{sudo} Adminrechte erlangen.
Danach wäre der Zugriff auf eventuell sensible private Daten (Cloudspeicher, HTWG home-Laufwerk, etc.) möglich.

Zur Verhinderung dieses Szenarios wird die LDAP-Authentifizierung durch die Zeile  \code{nss_base_passwd uid=<username>} auf den jeweiligen Benutzer beschränkt.
Somit wird die Suche für das Passwort eines Benutzer im LDAP-Verzeichnis gefiltert.

Schlussendlich sollen sich Benutzer über die gewohnte graphische Oberfläche (vgl. \autoref{lightdm}) am System anmelden können.
Hierzu muss der Login-Manager \emph{LightDM} konfiguriert werden und die Eingabe eines beliebigen Benutzernamens erlauben (ggü. dem Standard, der nur lokale Benutzer aus einer Liste zur Auswahl anbietet).
Dieses Ziel wurde erreicht, in dem die \code{SeatDefaults} in der entsprechenden Konfigurationsdatei angepasst wurden.

Da diese Installationen der Pakete normalerweise einen interaktiven Dialog erfordern, musste ein Weg gefunden werden, um dies automatisiert ausführen zu lassen.
Hier war der Befehl \code{DEBIAN_FRONTEND=noninteractive apt-get -y -q install ..} hilfreich. 
Dadurch werden bei interaktiven Dialogen die Standardwerte angenommen und alles weitere ohne Nachfrage installiert. 
Die anschließende manuelle Änderung der Konfigurationsdateien über ein Skript stellt die korrekten Einstellungen sicher.

Alle vorhergehenden Konfigurationen werden in den folgenden Dateien durchgeführt. 
Die genauen Details finden sich in den Readme und Skript Dateien bei den Projektquellen.

\begin{itemize}
\item \code{/etc/nsswitch.conf}: LDAP als PAM Authentifizierungsmodul.
\item \code{/etc/ldap.conf}: LDAP Konfiguration inkl. Attributmapping und -überschreiben.
\item \code{/etc/pam.d/common-session}: Erstellen des lokalen home Verzeichnisses beim erstmaligen Login.
\item \code{/etc/security/group.conf}: Zuweisung von lokalen Systemgruppen an LDAP Benutzer.
\item \code{/etc/pam.d/common-auth}: Hinzufügen des PAM Gruppenmoduls.
\item \code{/etc/group}: Definition der Systemgruppen für den jeweiligen LDAP-Benutzer.
\item \code{/etc/lightdm/lightdm.conf}: LDAP Login über den graphischen Login-Manager ermöglichen.
\end{itemize}

\subsection{HTWG home-Laufwerk}

Eine weitere Dienstleistung besteht darin, dem Benutzer sein gewohntes HTWG home-Laufwerk zur Verfügung zu stellen, um einen nahtlosen Wechsel zwischen OpenStack Instanz und lokalen Rechenzentrum-PCs sicherzustellen.

Das jeweilige home-Laufwerk unter \code{smb://homedrive.htwg-konstanz.de/home} erfordert eine Authentisierung des Nutzers gegenüber dem HTWG-Server. 
Jedoch soll aus Bequemlichkeitsgründen eine Single-Sign-On Lösung implementiert werden.
Der zentrale und einzige Login soll somit der Anmeldevorgang am Betriebssystem sein.

Als ideale Lösung stellt sich ein weiteres PAM-Modul heraus. \code{libpam_mount} bietet die gewünschte Funktionalität. 
Über zwei XML-Konfigurationsdateien wird das home-Laufwerk als \enquote{home-drive} unter \code{/media/home-drive} eingebunden.
Dadurch erscheint das Verzeichnis in der Seitenleiste des Nautilus Dateimanagers und ist komfortabel erreichbar.

\subsection{Block-Storage lab-drive}

OpenStack Instanzen können jederzeit neu gestartet oder erstellt werden.
Dies stellt die Verwaltung von benutzerspezifischen Dateien vor das Problem, auch über die Lebenszeit einer Instanz hinaus Daten speichern zu können. 

Die Block-Storage Komponente \emph{Cinder} bietet die Möglichkeit ein Stück Speicher mit Blockadressierung einer Instanz bereitzustellen. 
Dabei ist die Lebensdauer unabhängig von der Instanz. 
Dies erlaubt es, pro Übung eine neue Instanz zu starten mit jeweilig neuem Image, ohne wichtige Daten zu verlieren.

Zwar könnte grundsätzlich das home-Laufwerk der HTWG diese Stellung einnehmen, stellt aber aufgrund der sehr eingeschränkten Speicherkapazität keine zufriedenstellende Lösung dar.
Deshalb wird pro Instanz ein variabel großes Cinder-Laufwerk \enquote{lab-drive} erstellt und unter \code{/dev/vdb} angebunden.
Eine Herausforderung stellt die Hotplug-Fähigkeit dar. 
Das Laufwerk könnte zu jeder Zeit an die Instanz an- und abgeschlossen werden.
Des Weiteren muss beim ersten Anbinden das Laufwerk als ext4 formatiert werden.

Die Linux Systemdienste \code{mount (fstab)} und \code{udev} bieten die nötige Infrastruktur zur Realisierung.
Nachfolgend ist der Ablauf beschrieben, wie mit dem Laufwerk umgegangen wird bzw. die Tools installiert und konfiguriert werden.

\begin{itemize}
\item Einmaliges Erstellen des Mountpunktes \code{/media/lab-drive}.
\item Eintrag mit Mount-Informationen in \code{/etc/fstab} erstellen.
\item Ablages eines Skripts unter \code{/usr/local/bin/mount-lab-drive.sh}, welches das Mounten und Formatieren des Laufwerks übernimmt.
\item udev Regeldatei erstellen, die bei Änderungen im Linux Subsystem \code{block} das Mount-Skript aufruft.
\end{itemize}

Ebenso wie das home-Laufwerk erscheint das lab-drive in der Seitenleiste des Nautilus Dateimanagers.

\subsection{Installationsskript  für Benutzerkonfiguration}

Um die einzelnen Instanzen beim Erstellen auf einen Studenten anzupassen, muss das Image pro Student individuell verändert werden. 
Das \code{cloud-init} Paket bietet die Möglichkeit mittels dem Parameter \code{user-data} ein Skript beim Starten mitzugeben. 

Zur Installation der vorhergehend genannten Funktionen (LDAP-Authentifizierung etc.) wird ein generisches Skript generiert, welches nur noch minimal pro Student angepasst werden muss.
Dieses Skript \code{setup-instance.sh} wird durch ein Makefile erstellt, in dem alle \code{install.sh} Skripte der einzelnen Features aneinander gehängt und durch einen Header und Footer Teil ergänzt werden.
Im Header-Skript wird geprüft, ob root Rechte vorliegen und die \code{USER} Variable gesetzt und \code{apt-get} aktualisiert wurde. 
Der Footer ist dafür zuständig, nach dem Ausführen aller Teilskripte, den Rechner neuzustarten.
Die \code{USER} Variable muss vor dem Ausführen durch den tatsächlichen Rechenzentrum Benutzernamen des Studenten ersetzt werden.
Im Dashboard wird mittels Python das Skript angepasst und als \code{user-data} Parameter gesetzt.

\section{Probleme bei der Implementierung}

Da OpenStack rasant weiter entwickelt wird, bleibt an einigen Stellen die Dokumentation auf der Strecke bzw. ist veraltet. 
Herauszufinden, wie die Dienste konfiguriert werden müssen, war mit oftmals stundenlangen Recherchen verbunden.
Auch die Linux Module PAM mit LDAP sind sehr lückenhaft und veraltet dokumentiert.
Dies hat unweigerlich zu unzähligen Trial and Error Sitzungen geführt.
Meist resultiert die Lösung in nur wenigen Zeilen Code, diese jedoch mangels Dokumentation zu erstellen hat die Geschwindigkeit der Entwicklung stark gebremst.

Die Netzwerkfunktionalität von OpenStack ist einerseits extrem flexibel, zugleich aber auch extrem kompliziert und nur schwer zu durchdringen.
Eine lauffähige Netzwerkumgebung aufzusetzen, erforderte ebenfalls großen Aufwand.

Ein weiteres Problem bestand in der Erstellung des Basis-Image aufgrund von Performance Problemen.
Auf einem handelsüblichen Notebook eine virtuelle Maschine mit DevStack zu starten und in dieser dann nochmals ein komplettes Betriebssystem wie Ubuntu zu virtualisieren, zeigte der Hardware die Grenzen auf.
Deshalb musste zuerst ein VirtualBox Image konfiguriert und Skripte zur Umwandlung nach OpenStack entwickelt werden.

Insgesamt lässt sich mit einem so komplexen System wie OpenStack kein schneller Program-Compile-Execute Zyklus durchführen.

\section{Bekannte Bugs}

\begin{itemize}
\item Der LDAP-Login am Image muss zweimal durchgeführt werden. Hier liegt ein Bug seitens des LightDM Login-Managers vor.
\end{itemize}